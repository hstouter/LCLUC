{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a45686d",
   "metadata": {},
   "source": [
    "# LCLUC Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f43d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rioxarray as rioxr\n",
    "import xarray as xr\n",
    "import rasterstats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30327e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "wrk_dir = Path.cwd().parent # go up a directory with parent\n",
    "data_path = os.path.join(wrk_dir, 'data')\n",
    "\n",
    "all_label_data = pd.read_parquet(os.path.join(data_path, 'all_label_data.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abddbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set with only forest and non-forest labels \n",
    "def subset_labels(df, classes):\n",
    "    \"\"\"\n",
    "    Create dataset with specified classes, combining all others as 'nonforest'\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    subset = df.copy()\n",
    "    \n",
    "    # Replace class values: keep if in classes list, otherwise set to 'nonforest'\n",
    "    subset['class'] = subset['class'].apply(\n",
    "        lambda x: x if x in classes else 'nonforest'\n",
    "    )\n",
    "    \n",
    "    # Convert class column to categorical\n",
    "    subset['class'] = subset['class'].astype('category')\n",
    "    \n",
    "    return subset\n",
    "\n",
    "forest = subset_labels(all_label_data, ['forest'])\n",
    "forest_water_bare = subset_labels(all_label_data, ['forest', 'water', 'bare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c444362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "nonforest    1030\n",
      "forest        270\n",
      "Name: count, dtype: int64\n",
      "\n",
      "class\n",
      "nonforest    670\n",
      "bare         270\n",
      "forest       270\n",
      "water         90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"{forest['class'].value_counts()}\\n\")\n",
    "print(forest_water_bare['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1439b1",
   "metadata": {},
   "source": [
    "For the forest/non-forest, definitely have a class imbalance that should be considered in the model design..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf6ce16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class                      0\n",
       "coastal_areosol_L30       19\n",
       "blue_L30                  19\n",
       "green_L30                 19\n",
       "red_L30                   19\n",
       "NIR_narrow_L30            19\n",
       "SWIR1_L30                 19\n",
       "SWIR2_L30                 19\n",
       "cirrus_L30                19\n",
       "TIR1_L30                  19\n",
       "TIR2_L30                  19\n",
       "coastal_areosol_S30        0\n",
       "blue_S30                   0\n",
       "green_S30                  0\n",
       "red_S30                    0\n",
       "red_edge_1_S30             0\n",
       "red_edge_2_S30             0\n",
       "red_edge_3_S30             0\n",
       "NIR_broad_S30              0\n",
       "NIR_narrow_S30             0\n",
       "SWIR1_S30                  0\n",
       "SWIR2_S30                  0\n",
       "water_vapor_S30            0\n",
       "cirrus_S30                 0\n",
       "blue_evi_l30              19\n",
       "blue_evi_s30               0\n",
       "VV                         0\n",
       "VH                         0\n",
       "VV_VH                      0\n",
       "dem_band_0                 0\n",
       "dem_band_1                 0\n",
       "dem_band_2                 0\n",
       "dem_band_3                 0\n",
       "dem_band_4                 0\n",
       "dem_band_5                 0\n",
       "GLCM_mean_red              0\n",
       "GLCM_variance_red          0\n",
       "GLCM_homogeneity_red       0\n",
       "GLCM_contrast_red          0\n",
       "GLCM_dissimilarity_red     0\n",
       "GLCM_entropy_red           0\n",
       "GLCM_second_moment_red     0\n",
       "GLCM_correlation_red       0\n",
       "GLCM_mean_nir              0\n",
       "GLCM_variance_nir          0\n",
       "GLCM_homogeneity_nir       0\n",
       "GLCM_contrast_nir          0\n",
       "GLCM_dissimilarity_nir     0\n",
       "GLCM_entropy_nir           0\n",
       "GLCM_second_moment_nir     0\n",
       "GLCM_correlation_nir       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dde912",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "forest_clean = forest.drop(columns=['GLCM_correlation_nir', 'GLCM_contrast_red'])\n",
    "\n",
    "# Separate features and target\n",
    "X = forest_clean.drop(columns=['class'])\n",
    "y = forest_clean['class']\n",
    "\n",
    "# Scale to avoid warnings when imputing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "# Impute remaining NAs using KNN\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X_scaled), columns=X.columns)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42, stratify=y)  # Ensures both train/test have same class ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48750b0d",
   "metadata": {},
   "source": [
    "Here is where you can add more parameters and vary the ranges of the parameters. This is just a few to get started, but there are many more you can add and also other ways of optimizing a parameter grid beyond a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    # verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                feature  importance\n",
      "10  coastal_areosol_S30    0.167140\n",
      "24         blue_evi_s30    0.126484\n",
      "11             blue_S30    0.098665\n",
      "5             SWIR1_L30    0.067222\n",
      "2             green_L30    0.055974\n",
      "1              blue_L30    0.044298\n",
      "14       red_edge_1_S30    0.044013\n",
      "6             SWIR2_L30    0.036907\n",
      "20            SWIR2_S30    0.032166\n",
      "12            green_S30    0.031920\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance \n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac062dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 20,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what the best parameter selections were\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64fa3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model with best parameters\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    **grid_search.best_params_,\n",
    "    n_jobs=-1,\n",
    "    random_state = 808\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1797d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      forest       0.99      0.98      0.98        81\n",
      "   nonforest       0.99      1.00      1.00       309\n",
      "\n",
      "    accuracy                           0.99       390\n",
      "   macro avg       0.99      0.99      0.99       390\n",
      "weighted avg       0.99      0.99      0.99       390\n",
      "\n",
      "[[ 79   2]\n",
      " [  1 308]]\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c972811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. EFFICIENT Raster Prediction (avoids memory issues)\n",
    "# Stack rasters into numpy array\n",
    "raster_stack = rasters.values  # shape: (bands, y, x)\n",
    "n_bands, height, width = raster_stack.shape\n",
    "\n",
    "# Reshape to (n_pixels, n_bands)\n",
    "raster_2d = raster_stack.reshape(n_bands, -1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame with proper column names\n",
    "raster_df = pd.DataFrame(raster_2d, columns=rasters.band.values)\n",
    "\n",
    "# Apply same scaling transformation used during training\n",
    "raster_scaled = pd.DataFrame(\n",
    "    scaler.transform(raster_df),\n",
    "    columns=raster_df.columns\n",
    ")\n",
    "\n",
    "# Impute NAs in raster data\n",
    "raster_imputed = pd.DataFrame(\n",
    "    imputer.transform(raster_df), \n",
    "    columns=raster_df.columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Predict in Chunks (memory efficient for large rasters)\n",
    "chunk_size = 100000\n",
    "predictions = []\n",
    "\n",
    "for i in range(0, len(raster_imputed), chunk_size):\n",
    "    chunk = raster_imputed.iloc[i:i+chunk_size]\n",
    "    pred_chunk = rf_model.predict(chunk)\n",
    "    predictions.extend(pred_chunk)\n",
    "\n",
    "predictions = np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Recode to Numeric\n",
    "label_map = {'forest': 1, 'nonforest': 0}\n",
    "predictions_numeric = np.array([label_map.get(p, -1) for p in predictions])\n",
    "\n",
    "# 9. Reshape back to raster dimensions\n",
    "prediction_raster = predictions_numeric.reshape(height, width)\n",
    "\n",
    "# 10. Save as GeoTIFF\n",
    "output = rasters.isel(band=0).copy()  # Template from first band\n",
    "output.values = prediction_raster\n",
    "output.rio.to_raster(os.path.join(data_path, 'forest_prediction.tif'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
